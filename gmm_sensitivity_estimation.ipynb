{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gmm_sensitivity_estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dHkTQsRZng-4tdfczVuo_5h96gbpmx7p",
      "authorship_tag": "ABX9TyO4KQ64mdd/MRtqyDOqf3Ji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ColmTalbot/gmm_sensitivity_estimation/blob/main/gmm_sensitivity_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flexible and accurate evaluation of gravitational-wave Malmquist bias with machine learning\n",
        "\n",
        "This notebook is a companion to [Talbot and Thrane](https://arxiv.org/abs/2012.01317) that produced all results in that work.\n",
        "\n",
        "Note that the exact result will not in general be reproducible due to fluctuations in the random number generation.\n",
        "\n",
        "This notebook is as self contained as possible, however for the last section (performing population inference) a large amount of input data is required.\n",
        "Rather than download many GBs of data every time, I mount a local compressed version of the data. Instructions for how to obtain the full data set can be found at https://docs.ligo.org/RatesAndPopulations/gwpopulation_pipe/real-data.html although care should be taken in making sure all the correct events are identified."
      ],
      "metadata": {
        "id": "CMCFonHe8BLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "These first few cells will install all software needed to run this in [google colaboratory](colab.research.google.com).\n",
        "\n",
        "If you run with colab, I recommend using a GPU runtime.\n",
        "\n",
        "Some of the steps may not be necessary for some users or as updates get rolled out with colab. I make no guarantee that this will be the best way to use this going forward."
      ],
      "metadata": {
        "id": "qxuTMOfZ82pc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmVaWISnAsa"
      },
      "source": [
        "!pip install corner bilby gwpopulation astropy nestle gwpopulation_pipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm_FANSVPBjb"
      },
      "source": [
        "!pip uninstall matplotlib -y\n",
        "!pip install update matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RimHUX1mnH0Z"
      },
      "source": [
        "# !wget -nc https://zenodo.org/record/5636816/files/o1%2Bo2%2Bo3_bbhpop_real%2Bsemianalytic-LIGO-T2100377-v2.hdf5\n",
        "!wget -nc https://zenodo.org/record/5546676/files/endo3_bbhpop-LIGO-T2100113-v12.hdf5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install texlive-latex-recommended\n",
        "!sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended\n",
        "!wget http://mirrors.ctan.org/macros/latex/contrib/type1cm.zip\n",
        "!unzip type1cm.zip -d /tmp/type1cm\n",
        "!cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins\n",
        "!sudo mkdir /usr/share/texmf/tex/latex/type1cm\n",
        "!sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm\n",
        "!sudo texhash\n",
        "!apt install cm-super"
      ],
      "metadata": {
        "id": "njd2OIdWtDEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries\n",
        "\n",
        "The imports we'll use are here at the top for clarity.\n",
        "\n",
        "Also, various matplotlib settings to make nice plots."
      ],
      "metadata": {
        "id": "BICi5xGf9N-v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AINlBmb8nPdJ"
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "import os\n",
        "from copy import deepcopy\n",
        "import h5py\n",
        "\n",
        "import bilby\n",
        "from bilby.hyper.model import Model\n",
        "from bilby.core.prior import joint, PriorDict, Uniform\n",
        "import corner\n",
        "import gwpopulation\n",
        "import pandas as pd\n",
        "from bilby.hyper.model import Model\n",
        "from scipy.integrate import cumulative_trapezoid\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.special import betaln as scbetaln\n",
        "from scipy.stats import gaussian_kde, ks_2samp, norm, beta, truncnorm\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from tqdm.auto import tqdm, trange\n",
        "from gwpopulation.cupy_utils import to_numpy\n",
        "from gwpopulation.models.mass import two_component_primary_mass_ratio\n",
        "from gwpopulation.models.spin import iid_spin\n",
        "from gwpopulation.models.redshift import PowerLawRedshift\n",
        "from gwpopulation.vt import ResamplingVT\n",
        "from gwpopulation_pipe.utils import prior_conversion\n",
        "from gwpopulation_pipe.vt_helper import load_injection_data\n",
        "\n",
        "try:\n",
        "    import cupy as xp\n",
        "    from cupyx.scipy.special import erf, erfinv\n",
        "except ImportError:\n",
        "    xp = np\n",
        "    from scipy.special import erf, erfinv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARMiith9niHB"
      },
      "source": [
        "mpl.rcParams[\"font.family\"] = \"serif\"\n",
        "mpl.rcParams[\"font.serif\"] = \"Computer Modern Roman\"\n",
        "mpl.rcParams[\"font.size\"] = 20\n",
        "mpl.rcParams[\"text.usetex\"] = True\n",
        "mpl.rcParams[\"grid.alpha\"] = 0\n",
        "mpl.rcParams['text.latex.preamble'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxBdhRv2n6MU"
      },
      "source": [
        "### Load in the injection data\n",
        "\n",
        "Here we just use the found injections from the O3 injection campaign.\n",
        "\n",
        "The defails can be found on [zenodo](https://zenodo.org/record/5546676#.Yc80s_HP23I)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jvn7qXeGHTg"
      },
      "source": [
        "injections = load_injection_data(\"endo3_bbhpop-LIGO-T2100113-v12.hdf5\")\n",
        "fraction_not_hopeless = len(injections[\"mass_1\"]) / injections.pop(\"total_generated\")\n",
        "analysis_time = injections.pop(\"analysis_time\")\n",
        "for key in injections:\n",
        "    if isinstance(injections[key], xp.ndarray):\n",
        "        injections[key] = to_numpy(injections[key])\n",
        "all_found_injections = pd.DataFrame(injections)\n",
        "all_found_injections.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Empirical\" scaling preliminaries\n",
        "\n",
        "For our favoured scaling method we use a reflecting KDE of the found injections to construct an empirical CDF.\n",
        "\n",
        "This cells computes those distributions and verifies that the interpolated PDF agrees with the original samples and new samples we generate."
      ],
      "metadata": {
        "id": "waUtKufJ9o3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReflectingKDE(gaussian_kde):\n",
        "\n",
        "    def __init__(self, dataset, bw_method=None, weights=None, minimum=-np.inf, maximum=np.inf):\n",
        "        super(ReflectingKDE, self).__init__(\n",
        "            dataset=dataset, bw_method=bw_method, weights=weights\n",
        "        )\n",
        "        self.minimum = minimum\n",
        "        self.maximum = maximum\n",
        "        self.range = self.maximum - self.minimum\n",
        "\n",
        "    def __call__(self, points):\n",
        "        prob = self.evaluate(points)\n",
        "        fraction = (points - self.minimum) / self.range\n",
        "        if self.minimum > -np.inf:\n",
        "            prob += self.evaluate(self.minimum - fraction * self.range)\n",
        "            prob *= (points >= self.minimum)\n",
        "        if self.maximum < np.inf:\n",
        "            prob += self.evaluate(self.maximum + (1 - fraction) * self.range)\n",
        "            prob *= (points <= self.maximum)\n",
        "        return prob\n",
        "\n",
        "\n",
        "_data = deepcopy(all_found_injections)\n",
        "ranges = dict(\n",
        "    mass_1=(2, 100),\n",
        "    mass_ratio=(0, 1),\n",
        "    a_1=(0, 1),\n",
        "    a_2=(0, 1),\n",
        "    cos_tilt_1=(-1, 1),\n",
        "    cos_tilt_2=(-1, 1),\n",
        ")\n",
        "\n",
        "lnpdf_ = dict()\n",
        "cdf_ = dict()\n",
        "pdf_ = dict()\n",
        "ppf_ = dict()\n",
        "\n",
        "for key in ranges:\n",
        "    min_, max_ = ranges[key]\n",
        "    domain = np.linspace(min_, max_, 10000)\n",
        "    if key == \"mass_1\":\n",
        "        kde = ReflectingKDE(to_numpy(_data[key]), minimum=min_, maximum=max_, weights=to_numpy(_data[key] ** 0.5))\n",
        "    else:\n",
        "        kde = ReflectingKDE(to_numpy(_data[key]), minimum=min_, maximum=max_)\n",
        "\n",
        "    epdf = kde(domain)\n",
        "    if key == \"mass_1\":\n",
        "        epdf *= domain ** -0.5\n",
        "        epdf /= np.trapz(epdf, domain)\n",
        "\n",
        "    ecdf = cumulative_trapezoid(epdf, domain, initial=0)\n",
        "\n",
        "    pdf_[key] = interp1d(domain, epdf)\n",
        "    lnpdf_[key] = interp1d(domain, np.log(epdf))\n",
        "    cdf_[key] = interp1d(domain, ecdf)\n",
        "    ppf_[key] = interp1d(ecdf, domain)\n",
        "\n",
        "    plt.hist(_data[key], bins=100, density=True, histtype=\"step\")\n",
        "    plt.hist(ppf_[key](np.random.uniform(0, 1, 80000)), bins=100, density=True, histtype=\"step\")\n",
        "    plt.plot(domain, epdf)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "Y2--XbbWvV3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_smsP-aqkSI"
      },
      "source": [
        "### Injected distributions - mass\n",
        "\n",
        "We need to be able to simulate new events from the injected distribution.\n",
        "This involves defining some functions to do this.\n",
        "\n",
        "The mass distribution of the injections is\n",
        "\n",
        "$$\n",
        "p(m_1, m_2 | \\Lambda_0) = p(m_1 | \\Lambda_0) p(m_2 | m_1, \\Lambda_0) \\\\\n",
        "p(m_1 | \\Lambda) = \\frac{1}{(1 - \\alpha)} \\frac{m_1^{-\\alpha}}{(m_{\\max}^{1 - \\alpha} - m_{\\min}^{1 - \\alpha})} \\\\\n",
        "p(m_2 | m_1, \\Lambda) = \\frac{1}{(1 + \\beta)} \\frac{m_2^{\\beta}}{(m_{1}^{1 + \\beta} - m_{\\min}^{1 + \\beta})}.\n",
        "$$\n",
        "\n",
        "Later we will reparameterise in terms of the primary mass and the mass ratio.\n",
        "In that case the usual change of variables rule shows us\n",
        "$$\n",
        "p(m_1, q | \\Lambda_0) = p(m_1, m_2 | \\Lambda_0) \\left|\\frac{d m_2}{d q}\\right| = m_1 p(m_1, m_2 | \\Lambda_0).\n",
        "$$\n",
        "\n",
        "The specific hyperparameters used for the injections are\n",
        "$$\n",
        "\\Lambda_0 = \\{ \\alpha_{0}=-2.35, \\beta_{0}=2, m_{\\min, 0}=2, m_{\\max, 0}=100 \\}.\n",
        "$$\n",
        "\n",
        "All of these distributions are simply power laws.\n",
        "We need a few other features of this distribution namely the cumulative distribution function (CDF) and the percentile point function (PPF, the inverse of the CDF).\n",
        "For a generic parameter $x$ power law distributed in $[a, b]$ with index $k$ we have\n",
        "$$\n",
        "p(x | a, b, k) = \\frac{1}{(1 + k)} \\frac{x^{k}}{(b^{1 + k} - a^{1 + k})} \\\\\n",
        "CDF(x | a, b, k) = \\int_{a}^{x} dx' p(x' | a, b, k) = \\frac{(x^{1 + k} - a^{1 + k})}{(b^{1 + k} - a^{1 + k})} \\\\\n",
        "PPF(u | a, b, k) = \\left(a^{1 + k} + \\left(b^{1 + k} - a^{1 + k}\\right) u \\right)^{1 / (1 + k)} = a\\left(1 + \\left(\\left(\\frac{b}{a}\\right)^{1 + k} - 1\\right) u \\right)^{1 / (1 + k)}\n",
        "$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxQ7r3WKopXH"
      },
      "source": [
        "def powerlaw_cdf(value, alpha, minimum, maximum):\n",
        "    normalization = np.exp(powerlaw_ln_normalization(alpha, minimum, maximum))\n",
        "    return (value ** (1 + alpha) - minimum ** (1 + alpha)) * normalization / (1 + alpha)\n",
        "\n",
        "\n",
        "def powerlaw_ln_pdf(value, alpha, minimum, maximum):\n",
        "    return alpha * np.log(value) + powerlaw_ln_normalization(alpha, minimum, maximum)\n",
        "\n",
        "\n",
        "def powerlaw_ln_normalization(alpha, minimum, maximum):\n",
        "    return np.log((1 + alpha) / (maximum ** (1 + alpha) - minimum ** (1 + alpha)))\n",
        "\n",
        "\n",
        "def powerlaw_ppf(value, alpha, minimum, maximum):\n",
        "    \"\"\"\n",
        "    Percentile point function for a power law distribution.\n",
        "    \"\"\"\n",
        "    scale = (maximum / minimum) ** (alpha + 1)\n",
        "    return minimum * (1 + (scale - 1) * value) ** (1 / (alpha + 1))\n",
        "\n",
        "\n",
        "def powerlaw_sample(alpha, minimum, maximum, size=None, rng=xp.random):\n",
        "    x_rand = rng.uniform(0, 1, size=size)\n",
        "    parameters = dict(value=x_rand, alpha=alpha, minimum=minimum, maximum=maximum)\n",
        "    return powerlaw_ppf(**parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v11xCLFRzDiS"
      },
      "source": [
        "### Injected distributions - spin\n",
        "\n",
        "The injections are created with aligned spin components (\\chi) uniformly distribted in $[-1, 1]$.\n",
        "This is a special case of a power law with $k=0, a=-1, b=1$.\n",
        "We can simply write the PDF, CDF, and PPF in this case.\n",
        "$$\n",
        "p(\\chi | \\Lambda_0) = \\frac{1}{2} \\\\\n",
        "CDF(\\chi | \\Lambda_0) = \\frac{\\chi + 1}{2} \\\\\n",
        "PPF(u | \\Lambda_0) = 2 u - 1 \\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZI6Krqu1Coa"
      },
      "source": [
        "### Perform the initial mapping\n",
        "\n",
        "The distributions above are not conducive to fitting a Gaussian Mixture Model (GMM) to the data.\n",
        "This is primarily because they have sharp edges and finite domains.\n",
        "In order to mitigate this, we map the data onto an infinite domain in a two stage process\n",
        "$$\n",
        "x' = \\Phi^{-1}(U(x)).\n",
        "$$\n",
        "\n",
        "The first stage $G$ maps the physical domain to the unit interval $[0, 1]$.\n",
        "There are many possible ways to perform this mapping.\n",
        "For demonstration we consider four methods:\n",
        "- the first and simplest method performs a linear mapping over the domain $$U(x) = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}.$$\n",
        "- the next method uses the CDF of the injected distribution $$U(x) = \\int_{x_\\min}^{x} dx' \\, p(x' | \\Lambda_{0}).$$\n",
        "- the third method is an analytic approximation to the CDF of the observed distribution $$U(x) = \\int_{x_\\min}^{x} dx' \\, p(x' | \\Lambda_{0}) \\hat{p}_{\\rm det}(x') .$$\n",
        "- finally, we consider another case where we approximate the one-dimensional observed distributions using a kernel density estimate and construct an empirical CDF.\n",
        "\n",
        "For $\\hat{p}_{\\rm det}$ we choose the following\n",
        "$$\n",
        "\\hat{p}_{\\rm det}(m_1) = m_1^{2.35} \\\\\n",
        "\\hat{p}_{\\rm det}(q) = q^2\n",
        "$$\n",
        "The mass scaling follows the leading order dependence of signal amplitude with mass and has been discussed many places in the literature.\n",
        "The mass ratio scaling is somewhat ad hoc based on numerical tests of what makes the scaled samples look closest to a uniform disribution on $[0, 1]$.\n",
        "\n",
        "The second stage maps from the unit interval onto the inifite domain $[-\\infty, \\infty]$ using the probit function (the inverse of the CDF of the unit normal distribution.)\n",
        "As long as the underlying distribution doesn't rail exponentially against the edge of the original domain, after this transformation the data will go to zero smoothly at positive and negative infinity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC2-uSnq07gA"
      },
      "source": [
        "def naive_scale(values, minimum, maximum):\n",
        "    return (values - minimum) / (maximum - minimum)\n",
        "\n",
        "\n",
        "def naive_unscale(values, minimum, maximum):\n",
        "    return values * (maximum - minimum) + minimum\n",
        "\n",
        "\n",
        "def call_interp(point, interpolant):\n",
        "    if not hasattr(interpolant, \"xcp\"):\n",
        "        interpolant.xcp = xp.asarray(interpolant.x)\n",
        "    if not hasattr(interpolant, \"ycp\"):\n",
        "        interpolant.ycp = xp.asarray(interpolant.y)\n",
        "    delta_x = interpolant.xcp[1] - interpolant.xcp[0]\n",
        "    idx = (point - interpolant.xcp[0]) // delta_x\n",
        "    idx = idx.astype(int)\n",
        "    fraction = (point - interpolant.xcp[0]) % delta_x\n",
        "    return (\n",
        "        (1 - fraction) * interpolant.ycp[idx]\n",
        "        + fraction * interpolant.ycp[idx + 1]\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def scale_data(data, parameters, scale_method=\"cdf\", module=xp):\n",
        "    data = deepcopy(data)\n",
        "\n",
        "    if \"minimum_mass\" in parameters:\n",
        "        minimum_mass = parameters[\"minimum_mass\"]\n",
        "    else:\n",
        "        minimum_mass = parameters[\"mmin\"]\n",
        "    if \"maximum_mass\" in parameters:\n",
        "        maximum_mass = parameters[\"maximum_mass\"]\n",
        "    else:\n",
        "        maximum_mass = parameters[\"mmax\"]\n",
        "    minimum_spin = parameters[\"minimum_spin\"]\n",
        "    maximum_spin = parameters[\"maximum_spin\"]\n",
        "\n",
        "    if scale_method == \"cdf\":\n",
        "\n",
        "        data[\"a_1\"] = naive_scale(data[\"a_1\"], 0, maximum_spin)\n",
        "        data[\"a_2\"] = naive_scale(data[\"a_2\"], 0, maximum_spin)\n",
        "        data[\"cos_tilt_1\"] = naive_scale(data[\"cos_tilt_1\"], minimum_spin, maximum_spin)\n",
        "        data[\"cos_tilt_2\"] = naive_scale(data[\"cos_tilt_2\"], minimum_spin, maximum_spin)\n",
        "\n",
        "        data[\"mass_ratio\"] = powerlaw_cdf(\n",
        "            data[\"mass_ratio\"],\n",
        "            alpha=parameters[\"beta\"],\n",
        "            minimum=minimum_mass / data[\"mass_1\"],\n",
        "            maximum=1,\n",
        "        )\n",
        "\n",
        "        data[\"mass_1\"] = powerlaw_cdf(\n",
        "            data[\"mass_1\"],\n",
        "            minimum=minimum_mass,\n",
        "            maximum=maximum_mass,\n",
        "            alpha=parameters[\"alpha\"] + 0\n",
        "        )\n",
        "\n",
        "    elif scale_method == \"naive\":\n",
        "\n",
        "        data[\"a_1\"] = naive_scale(data[\"a_1\"], 0, maximum_spin)\n",
        "        data[\"a_2\"] = naive_scale(data[\"a_2\"], 0, maximum_spin)\n",
        "        data[\"cos_tilt_1\"] = naive_scale(data[\"cos_tilt_1\"], minimum_spin, maximum_spin)\n",
        "        data[\"cos_tilt_2\"] = naive_scale(data[\"cos_tilt_2\"], minimum_spin, maximum_spin)\n",
        "\n",
        "        data[\"mass_ratio\"] = naive_scale(data[\"mass_ratio\"], minimum_mass / data[\"mass_1\"], 1)\n",
        "        data[\"mass_1\"] = naive_scale(data[\"mass_1\"], minimum_mass, maximum_mass)\n",
        "\n",
        "\n",
        "    elif scale_method == \"empirical\":\n",
        "        for key in data:\n",
        "            if module == np:\n",
        "                data[key] = cdf_[key](data[key])\n",
        "            else:\n",
        "                data[key] = call_interp(xp.asarray(data[key]), cdf_[key])\n",
        "            \n",
        "    elif scale_method == \"approximate\":\n",
        "        data[\"a_1\"] = naive_scale(data[\"a_1\"], 0, maximum_spin)\n",
        "        data[\"a_2\"] = naive_scale(data[\"a_2\"], 0, maximum_spin)\n",
        "        data[\"cos_tilt_1\"] = naive_scale(data[\"cos_tilt_1\"], minimum_spin, maximum_spin)\n",
        "        data[\"cos_tilt_2\"] = naive_scale(data[\"cos_tilt_2\"], minimum_spin, maximum_spin)\n",
        "\n",
        "        data[\"mass_ratio\"] = powerlaw_cdf(\n",
        "            data[\"mass_ratio\"],\n",
        "            alpha=parameters[\"beta\"] + 2,\n",
        "            minimum=minimum_mass / data[\"mass_1\"],\n",
        "            maximum=1,\n",
        "        )\n",
        "\n",
        "        data[\"mass_1\"] = powerlaw_cdf(\n",
        "            data[\"mass_1\"],\n",
        "            minimum=minimum_mass,\n",
        "            maximum=maximum_mass,\n",
        "            alpha=parameters[\"alpha\"] + 2.35\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid input {scaled_method}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def unscale_data(data, parameters, scale_method=\"cdf\"):\n",
        "    data = deepcopy(data)\n",
        "\n",
        "    if \"minimum_mass\" in parameters:\n",
        "        minimum_mass = parameters[\"minimum_mass\"]\n",
        "    else:\n",
        "        minimum_mass = parameters[\"mmin\"]\n",
        "    if \"maximum_mass\" in parameters:\n",
        "        maximum_mass = parameters[\"maximum_mass\"]\n",
        "    else:\n",
        "        maximum_mass = parameters[\"mmax\"]\n",
        "\n",
        "    if scale_method == \"cdf\":\n",
        "\n",
        "        data[\"a_1\"] = naive_unscale(data[\"a_1\"], 0, maximum_spin)\n",
        "        data[\"a_2\"] = naive_unscale(data[\"a_2\"], 0, maximum_spin)\n",
        "        data[\"cos_tilt_1\"] = naive_unscale(data[\"cos_tilt_1\"], minimum_spin, maximum_spin)\n",
        "        data[\"cos_tilt_2\"] = naive_unscale(data[\"cos_tilt_2\"], minimum_spin, maximum_spin)\n",
        "\n",
        "        data[\"mass_1\"] = powerlaw_ppf(\n",
        "            data[\"mass_1\"],\n",
        "            minimum=minimum_mass,\n",
        "            maximum=maximum_mass,\n",
        "            alpha=parameters[\"alpha\"] + 0\n",
        "        )\n",
        "\n",
        "        data[\"mass_ratio\"] = powerlaw_ppf(\n",
        "            data[\"mass_ratio\"],\n",
        "            alpha=parameters[\"beta\"],\n",
        "            minimum=minimum_mass / data[\"mass_1\"],\n",
        "            maximum=1,\n",
        "        )\n",
        "\n",
        "    elif scale_method == \"naive\":\n",
        "\n",
        "        data[\"a_1\"] = naive_unscale(data[\"a_1\"], 0, maximum_spin)\n",
        "        data[\"a_2\"] = naive_unscale(data[\"a_2\"], 0, maximum_spin)\n",
        "        data[\"cos_tilt_1\"] = naive_unscale(data[\"cos_tilt_1\"], minimum_spin, maximum_spin)\n",
        "        data[\"cos_tilt_2\"] = naive_unscale(data[\"cos_tilt_2\"], minimum_spin, maximum_spin)\n",
        "\n",
        "        data[\"mass_1\"] = naive_unscale(data[\"mass_1\"], minimum_mass, maximum_mass)\n",
        "        data[\"mass_ratio\"] = naive_unscale(data[\"mass_ratio\"], minimum_mass / data[\"mass_1\"], 1)\n",
        "\n",
        "    elif scale_method == \"empirical\":\n",
        "\n",
        "        for key in data:\n",
        "            data[key] = xp.asarray(ppf_[key](to_numpy(data[key])))\n",
        "\n",
        "    elif scale_method == \"approximate\":\n",
        "        data[\"a_1\"] = naive_unscale(data[\"a_1\"], 0, maximum_spin)\n",
        "        data[\"a_2\"] = naive_unscale(data[\"a_2\"], 0, maximum_spin)\n",
        "        data[\"cos_tilt_1\"] = naive_unscale(data[\"cos_tilt_1\"], minimum_spin, maximum_spin)\n",
        "        data[\"cos_tilt_2\"] = naive_unscale(data[\"cos_tilt_2\"], minimum_spin, maximum_spin)\n",
        "\n",
        "        data[\"mass_1\"] = powerlaw_ppf(\n",
        "            data[\"mass_1\"],\n",
        "            minimum=minimum_mass,\n",
        "            maximum=maximum_mass,\n",
        "            alpha=parameters[\"alpha\"] + 2.35\n",
        "        )\n",
        "\n",
        "        data[\"mass_ratio\"] = powerlaw_ppf(\n",
        "            data[\"mass_ratio\"],\n",
        "            alpha=parameters[\"beta\"] + 2,\n",
        "            minimum=minimum_mass / data[\"mass_1\"],\n",
        "            maximum=1,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid input {scaled_method}\")\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the original and scaled data.\n",
        "\n",
        "This is Figure 1 of the paper."
      ],
      "metadata": {
        "id": "-NKHfCUf_HpY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anZpiAFe8dt2"
      },
      "source": [
        "from corner import hist2d as chist2d\n",
        "\n",
        "_labels = dict(\n",
        "    mass_1=\"$m_{1}$\",\n",
        "    mass_ratio=\"$q$\",\n",
        "    a_1=\"$a_1$\",\n",
        "    a_2=\"$a_2$\",\n",
        "    cos_tilt_1=\"$\\\\cos\\\\theta_1$\",\n",
        "    cos_tilt_2=\"$\\\\cos\\\\theta_2$\",\n",
        "    chi_1=\"$\\\\chi_1$\",\n",
        "    chi_2=\"$\\\\chi_2$\",\n",
        ")\n",
        "\n",
        "injection_parameters = dict(\n",
        "    alpha=-2.35,\n",
        "    beta=1,\n",
        "    minimum_mass=2,\n",
        "    maximum_mass=100,\n",
        "    minimum_spin=-1,\n",
        "    maximum_spin=1,\n",
        ")\n",
        "\n",
        "mpl.rcParams[\"font.size\"] = 30\n",
        "\n",
        "fit_keys = [\"mass_1\", \"mass_ratio\", \"a_1\", \"a_2\", \"cos_tilt_1\", \"cos_tilt_2\"]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=5, ncols=len(fit_keys) - 1, figsize=(5 * len(fit_keys) - 1, 25))\n",
        "\n",
        "for ii in range(len(fit_keys) - 1):\n",
        "    key_1 = fit_keys[ii]\n",
        "    key_2 = fit_keys[ii + 1]\n",
        "    chist2d(\n",
        "        all_found_injections[key_1].values, all_found_injections[key_2].values,\n",
        "        bins=50, smooth=True, color=\"C0\",\n",
        "        plot_density=False, plot_datapoints=False, fill_contours=True,\n",
        "        ax=axs[0][ii],\n",
        "    )\n",
        "    axs[0][ii].set_xlabel(_labels[key_1])\n",
        "    axs[0][ii].set_ylabel(_labels[key_2])\n",
        "\n",
        "scaled_data = dict()\n",
        "scalings = dict(naive=\"Naive\", cdf=\"CDF\", approximate=\"Approximate\", empirical=\"Empirical\")\n",
        "\n",
        "for jj, scale_method in enumerate([\"naive\", \"cdf\", \"approximate\", \"empirical\"]):\n",
        "\n",
        "    data = scale_data(\n",
        "        all_found_injections.copy()[fit_keys],\n",
        "        parameters=injection_parameters,\n",
        "        scale_method=scale_method,\n",
        "        module=np,\n",
        "    )\n",
        "    scaled_data[scale_method] = data\n",
        "    original_samples = norm.ppf(np.array(data[fit_keys].values))\n",
        "\n",
        "    for ii in range(len(fit_keys) - 1):\n",
        "        key_1 = fit_keys[ii]\n",
        "        key_2 = fit_keys[ii + 1]\n",
        "        chist2d(\n",
        "            original_samples[:, ii], original_samples[:, ii + 1], \n",
        "            bins=50, smooth=True, color=f\"C{jj + 1}\",\n",
        "            plot_density=False, plot_datapoints=False, fill_contours=True,\n",
        "            ax=axs[jj + 1][ii],\n",
        "        )\n",
        "        axs[jj + 1][ii].set_xlabel(f\"{scalings[scale_method]} {_labels[key_1]}\")\n",
        "        axs[jj + 1][ii].set_ylabel(f\"{scalings[scale_method]} {_labels[key_2]}\")\n",
        "        axs[jj + 1][ii].set_xlim(-3, 3)\n",
        "        axs[jj + 1][ii].set_ylim(-3, 3)\n",
        "\n",
        "axs[0][0].set_xlabel(\"$m_{1} [M_{\\\\odot}]$\")\n",
        "axs[1][1].set_xlim(-1, 3)\n",
        "axs[2][0].set_xlim(0, 4)\n",
        "axs[1][0].set_ylim(-1, 3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"scaling-comparison.pdf\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "mpl.rcParams[\"font.size\"] = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also look at the distribution of the found injections after the initial unit mapping. This tells us how well each method does at approximating the observed one-dimensional distributions."
      ],
      "metadata": {
        "id": "1qJ8_eTz_NLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key in scaled_data:\n",
        "    print(scalings[key])\n",
        "    for parameter in scaled_data[key][fit_keys]:\n",
        "        plt.hist(scaled_data[key][parameter], bins=100, density=True, histtype=\"step\", label=parameter)\n",
        "        plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "dFU1wmum07jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9v-dgVAWw11"
      },
      "source": [
        "### Find best fit\n",
        "\n",
        "Now we have scaled the injections to the fitting space, we can fit the GMMs to the data.\n",
        "We use the GMM implementation in `scikit-learn`.\n",
        "\n",
        "In order to identify a suitable number of components to use, we could use any of the three most widely used metrics implemented in `scikit-learn`:\n",
        "- the score is the mean natural log likelihood over a set of input samples, $S = \\left< \\ln \\mathcal{L} \\right>.$\n",
        "- the Akaike information criterion, $AIC = 2 k - 2 \\ln \\mathcal{L}.$\n",
        "- the Bayesian information criterion, $BIC = k \\ln N - 2 \\ln \\mathcal{L}.$\n",
        "\n",
        "The first is maximized for the \"best\" model is independent of the number of samples (once a sufficient number of samples have been included) and carries no penalty for using a larger model and so may be more likely to overfit the data.\n",
        "\n",
        "The other two metrics include a fixed correction for the complexity of the model and are minimized for the \"best\" model.\n",
        "The parameter $k$ is the number of parameters of the model and N is the number of input samples.\n",
        "\n",
        "We use the first metric, however, we split the data into a testing and training set so that we can identify any overfitting.\n",
        "\n",
        "This cell generates Figure 2 of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSYp-K8t_0Zo"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
        "\n",
        "for ii, scale_method in enumerate([\"empirical\"]):\n",
        "    data = scaled_data[scale_method]\n",
        "    original_samples = norm.ppf(np.array(data[fit_keys[:]].values))\n",
        "\n",
        "    n_train = int(len(original_samples) * 0.8)\n",
        "    n_test = len(original_samples) - n_train\n",
        "    training = np.random.choice(len(original_samples), n_train, replace=False)\n",
        "    train = np.zeros(len(original_samples), dtype=bool)\n",
        "    train[training] = True\n",
        "    test = ~train\n",
        "\n",
        "    train_samples = original_samples[train]\n",
        "    test_samples = original_samples[test]\n",
        "    train_scores = list()\n",
        "    test_scores = list()\n",
        "    all_components = range(1, 21, 1)\n",
        "\n",
        "    for n_components in tqdm(all_components):\n",
        "        gmm = GaussianMixture(n_components=n_components).fit(train_samples)\n",
        "        train_scores.append(gmm.score(train_samples))\n",
        "        test_scores.append(gmm.score(test_samples))\n",
        "\n",
        "    plt.sca(axes)\n",
        "    plt.scatter(all_components, train_scores, label=\"Train\", marker=\"+\", s=40, color=f\"C0\")\n",
        "    plt.scatter(all_components, test_scores, label=\"Test\", marker=\"x\", s=40, color=f\"C1\")\n",
        "    plt.xlim(1, n_components)\n",
        "    plt.xlabel(\"N components\")\n",
        "    plt.ylabel(\"$\\\\left<\\\\ln \\\\mathcal{D}\\\\right>$\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"scaling-vs-components.pdf\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are helper functions that will perform the bulk of the analysis.\n",
        "\n",
        "Many of these are methods to call Gaussian mixture models that is optimized to our purposes (GPU compatibility, caching repeated steps, etc) and closely adapted from `sklearn` code."
      ],
      "metadata": {
        "id": "_C6pKcLP__YF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcDxj7umOog3"
      },
      "source": [
        "def build_gmms(n_components, kinds=None):\n",
        "    if kinds is None:\n",
        "        kinds = list(scaled_data.keys())\n",
        "    gmms = dict()\n",
        "\n",
        "    for scale_method in kinds:\n",
        "        original_samples = norm.ppf(np.array(scaled_data[scale_method][fit_keys[:]].values))\n",
        "        gmm = GaussianMixture(n_components=n_components).fit(original_samples)\n",
        "        new_gmm = deepcopy(gmm)\n",
        "        gmms[scale_method] = CustomGMM(gmm)\n",
        "    return gmms, test\n",
        "\n",
        "\n",
        "def draw_samples(scale_method, n_samples=None):\n",
        "    if n_samples is None:\n",
        "        n_samples = len(original_samples)\n",
        "    new_gmm = gmms[scale_method]\n",
        "    new_samples = unit_normal_cdf(new_gmm.sample(len(original_samples))[0])\n",
        "\n",
        "    new_data = {\n",
        "        key: new_samples[:, ii] for ii, key in enumerate(fit_keys)\n",
        "    }\n",
        "    new_data = unscale_data(\n",
        "        new_data.copy(),\n",
        "        parameters=injection_parameters,\n",
        "        scale_method=scale_method\n",
        "    )\n",
        "    return new_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ypO8a5QQ044"
      },
      "source": [
        "def truncnorm_sample(n_samples, mu, sigma, minimum, maximum):\n",
        "    val = xp.random.uniform(0, 1, n_samples)\n",
        "    normalisation = (\n",
        "        erf((maximum - mu) / 2 ** 0.5 / sigma)\n",
        "        - erf((minimum - mu) / 2 ** 0.5 / sigma)\n",
        "    ) / 2\n",
        "    return erfinv(\n",
        "        2 * val * normalisation\n",
        "        + erf((minimum - mu) / 2 ** 0.5 / sigma)\n",
        "    ) * 2 ** 0.5 * sigma + mu\n",
        "\n",
        "\n",
        "def unit_normal_cdf(xx):\n",
        "    return (1 + erf(xx / 2 ** 0.5)) / 2\n",
        "\n",
        "\n",
        "def unit_normal_ppf(xx):\n",
        "    return 2 ** 0.5 * erfinv(2 * xx - 1)\n",
        "\n",
        "\n",
        "def logsumexp(a, axis=None, b=None, keepdims=False, return_sign=False):\n",
        "    \"\"\"\n",
        "    Copied almost verbatim from\n",
        "    https://github.com/scipy/scipy/blob/v1.5.2/scipy/special/_logsumexp.py#L7-L127\n",
        "    \"\"\"\n",
        "    a_max = xp.amax(a, axis=axis, keepdims=True)\n",
        "\n",
        "    if a_max.ndim > 0:\n",
        "        a_max[~xp.isfinite(a_max)] = 0\n",
        "    elif not xp.isfinite(a_max):\n",
        "        a_max = 0\n",
        "\n",
        "    if b is not None:\n",
        "        b = xp.asarray(b)\n",
        "        tmp = b * xp.exp(a - a_max)\n",
        "    else:\n",
        "        tmp = xp.exp(a - a_max)\n",
        "\n",
        "    s = xp.sum(tmp, axis=axis, keepdims=keepdims)\n",
        "    if return_sign:\n",
        "        sgn = xp.sign(s)\n",
        "        s *= sgn  # /= makes more sense but we need zero -> zero\n",
        "    out = xp.log(s)\n",
        "\n",
        "    if not keepdims:\n",
        "        a_max = xp.squeeze(a_max, axis=axis)\n",
        "    out += a_max\n",
        "\n",
        "    if return_sign:\n",
        "        return out, sgn\n",
        "    else:\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def _estimate_gmm_log_gaussian_prob(X, precisions_chol_, means_inner_precisions):\n",
        "    \"\"\"Estimate the log Gaussian probability.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "    means : array-like, shape (n_components, n_features)\n",
        "    precisions_chol : array-like\n",
        "        Cholesky decompositions of the precision matrices.\n",
        "        'full' : shape of (n_components, n_features, n_features)\n",
        "        'tied' : shape of (n_features, n_features)\n",
        "        'diag' : shape of (n_components, n_features)\n",
        "        'spherical' : shape of (n_components,)\n",
        "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
        "    Returns\n",
        "    -------\n",
        "    log_prob : array, shape (n_samples, n_components)\n",
        "    \"\"\"\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(1, -1)\n",
        "    _, n_features = X.shape\n",
        "\n",
        "    log_prob = xp.sum(xp.square(\n",
        "        xp.tensordot(X, precisions_chol_, axes=([1], [1]))\n",
        "        - means_inner_precisions\n",
        "    ), axis=-1)\n",
        "\n",
        "    return -.5 * (n_features * np.log(2 * np.pi) + log_prob)\n",
        "\n",
        "\n",
        "def _compute_log_det_cholesky(matrix_chol):\n",
        "    \"\"\"Compute the log-det of the cholesky decomposition of matrices.\n",
        "    Parameters\n",
        "    ----------\n",
        "    matrix_chol : array-like\n",
        "        Cholesky decompositions of the matrices.\n",
        "        'full' : shape of (n_components, n_features, n_features)\n",
        "        'tied' : shape of (n_features, n_features)\n",
        "        'diag' : shape of (n_components, n_features)\n",
        "        'spherical' : shape of (n_components,)\n",
        "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
        "    Returns\n",
        "    -------\n",
        "    log_det_precision_chol : array-like, shape (n_components,)\n",
        "        The determinant of the precision matrix for each component.\n",
        "    \"\"\"\n",
        "    n_components, n_features, _ = matrix_chol.shape\n",
        "    log_det_chol = xp.sum(xp.log(\n",
        "        matrix_chol.reshape(n_components, -1)[:, ::n_features + 1]\n",
        "    ), 1)\n",
        "\n",
        "    return log_det_chol\n",
        "\n",
        "\n",
        "class CustomGMM(object):\n",
        "    def __init__(self, gmm, normalization=1):\n",
        "        self.gmm = gmm\n",
        "        self.precisions_cholesky_ = xp.asarray(self.gmm.precisions_cholesky_)\n",
        "        self._logdet = _compute_log_det_cholesky(self.precisions_cholesky_)\n",
        "        self.means_ = xp.asarray(self.gmm.means_)\n",
        "        self.ln_weights_ = xp.log(xp.asarray(self.gmm.weights_))\n",
        "        self.ln_factor = xp.log(1 / normalization)\n",
        "        self._means_dot_precisions_ = xp.einsum(\n",
        "            \"ik,ikj->ij\", self.means_, self.precisions_cholesky_\n",
        "        )\n",
        "\n",
        "    def sample(self, n_samples=1):\n",
        "        samples, components = self.gmm.sample(n_samples=n_samples)\n",
        "        return xp.asarray(samples), xp.asarray(components)\n",
        "\n",
        "    def score_samples(self, samples):\n",
        "        return self.ln_factor + logsumexp(\n",
        "            _estimate_gmm_log_gaussian_prob(\n",
        "                samples, self.precisions_cholesky_, self._means_dot_precisions_\n",
        "            ) + self._logdet + self.ln_weights_,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "\n",
        "def unit_normal_ln_pdf(xx):\n",
        "    return - xx ** 2 / 2 - np.log(2 * np.pi) / 2\n",
        "\n",
        "\n",
        "def ln_pdf_naive(data, parameters, gmm):\n",
        "    scaled_data = scale_data(\n",
        "        data.copy(),\n",
        "        parameters=parameters,\n",
        "        scale_method=\"naive\"\n",
        "    )\n",
        "    scaled_data = xp.asarray([\n",
        "        unit_normal_ppf(scaled_data[key])\n",
        "        for key in scaled_data\n",
        "    ]).T\n",
        "    ln_jacobian = (\n",
        "        - xp.log(parameters[\"maximum_mass\"] - parameters[\"minimum_mass\"])\n",
        "        - xp.log(1 - parameters[\"minimum_mass\"] / data[\"mass_1\"])\n",
        "        - powerlaw_ln_pdf(\n",
        "            data[\"mass_1\"],\n",
        "            parameters[\"alpha\"],\n",
        "            parameters[\"minimum_mass\"],\n",
        "            parameters[\"maximum_mass\"]\n",
        "        ) - powerlaw_ln_pdf(\n",
        "            data[\"mass_ratio\"],\n",
        "            parameters[\"beta\"],\n",
        "            parameters[\"minimum_mass\"] / data[\"mass_1\"],\n",
        "            1\n",
        "        )\n",
        "    )\n",
        "    return (\n",
        "        gmm.score_samples(scaled_data)\n",
        "        - xp.sum(unit_normal_ln_pdf(scaled_data), axis=-1)\n",
        "        + ln_jacobian\n",
        "    )\n",
        "\n",
        "\n",
        "def ln_pdf_cdf(data, parameters, gmm):\n",
        "    scaled_data = scale_data(\n",
        "        data.copy(),\n",
        "        parameters=parameters,\n",
        "        scale_method=\"cdf\"\n",
        "    )\n",
        "    scaled_data = xp.asarray([\n",
        "        unit_normal_ppf(scaled_data[key])\n",
        "        for key in scaled_data\n",
        "    ]).T\n",
        "    return (\n",
        "        gmm.score_samples(scaled_data)\n",
        "        - xp.sum(unit_normal_ln_pdf(scaled_data), axis=-1)\n",
        "    )\n",
        "\n",
        "\n",
        "def ln_pdf_approximate(data, parameters, gmm):\n",
        "    scaled_data = scale_data(\n",
        "        data.copy(),\n",
        "        parameters=parameters,\n",
        "        scale_method=\"approximate\"\n",
        "    )\n",
        "    scaled_data = xp.asarray([\n",
        "        unit_normal_ppf(scaled_data[key])\n",
        "        for key in scaled_data\n",
        "    ]).T\n",
        "    return (\n",
        "        gmms[\"approximate\"].score_samples(scaled_data)\n",
        "        - xp.sum(unit_normal_ln_pdf(scaled_data), axis=-1)\n",
        "        + 2.35 * xp.log(data[\"mass_1\"])\n",
        "        + powerlaw_ln_normalization(\n",
        "            parameters[\"alpha\"] + 2.35, parameters[\"minimum_mass\"], parameters[\"maximum_mass\"]\n",
        "        )\n",
        "        - powerlaw_ln_normalization(\n",
        "            parameters[\"alpha\"], parameters[\"minimum_mass\"], parameters[\"maximum_mass\"]\n",
        "        )\n",
        "        + 2 * np.log(data[\"mass_ratio\"])\n",
        "        + powerlaw_ln_normalization(\n",
        "            parameters[\"beta\"] + 2, parameters[\"minimum_mass\"] / data[\"mass_1\"], 1\n",
        "        )\n",
        "        - powerlaw_ln_normalization(\n",
        "            parameters[\"beta\"], parameters[\"minimum_mass\"] / data[\"mass_1\"], 1\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def ln_pdf_empirical(data, parameters, gmm):\n",
        "    scaled_data = scale_data(\n",
        "        data.copy(),\n",
        "        parameters=parameters,\n",
        "        scale_method=\"empirical\"\n",
        "    )\n",
        "    ln_jacobian = (\n",
        "        call_interp(data[\"mass_1\"], lnpdf_[\"mass_1\"])\n",
        "        + call_interp(data[\"mass_ratio\"], lnpdf_[\"mass_ratio\"])\n",
        "        + call_interp(data[\"a_1\"], lnpdf_[\"a_1\"])\n",
        "        + call_interp(data[\"a_2\"], lnpdf_[\"a_2\"])\n",
        "        + call_interp(data[\"cos_tilt_1\"], lnpdf_[\"cos_tilt_1\"])\n",
        "        + call_interp(data[\"cos_tilt_2\"], lnpdf_[\"cos_tilt_2\"])\n",
        "        - powerlaw_ln_pdf(\n",
        "            data[\"mass_1\"],\n",
        "            parameters[\"alpha\"],\n",
        "            parameters[\"minimum_mass\"],\n",
        "            parameters[\"maximum_mass\"]\n",
        "        ) - powerlaw_ln_pdf(\n",
        "            data[\"mass_ratio\"],\n",
        "            parameters[\"beta\"],\n",
        "            parameters[\"minimum_mass\"] / data[\"mass_1\"],\n",
        "            1\n",
        "        )\n",
        "        + np.log(4)\n",
        "    )\n",
        "    scaled_data = xp.asarray([\n",
        "        unit_normal_ppf(scaled_data[key])\n",
        "        for key in scaled_data\n",
        "    ]).T\n",
        "    return (\n",
        "        gmms[\"empirical\"].score_samples(scaled_data)\n",
        "        - xp.sum(unit_normal_ln_pdf(scaled_data), axis=-1)\n",
        "        + ln_jacobian\n",
        "    )\n",
        "        \n",
        "\n",
        "def ln_pdet(data, parameters, gmms, scale_method):\n",
        "    funcs = dict(\n",
        "        naive=ln_pdf_naive, cdf=ln_pdf_cdf, empirical=ln_pdf_empirical, approximate=ln_pdf_approximate\n",
        "    )\n",
        "    if scale_method not in funcs:\n",
        "        raise KeyError(\n",
        "            f\"Scaling method {scale_method} not recognized, should be in {', '.join(funcs.keys())}.\"\n",
        "        )\n",
        "    return funcs[scale_method](data, parameters, gmms[scale_method]) + np.log(fraction_not_hopeless)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the population likelihood\n",
        "\n",
        "We're finally ready to start evaluating the some population-averaged selection functions.\n",
        "\n",
        "This uses a mixture of existing code and the functions defined above to do all of the calculations. For details of how these work, see Section 2 of the paper.\n",
        "\n",
        "First we set up the old style method where we directly resampling the found injections."
      ],
      "metadata": {
        "id": "s8FmLui9Avc-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuuiGoQesuJY",
        "outputId": "9d8ca7bd-f501-49b3-b00a-e3adfb75a0dc"
      },
      "source": [
        "model = Model([two_component_primary_mass_ratio, iid_spin])\n",
        "\n",
        "_data = load_injection_data(\"endo3_bbhpop-LIGO-T2100113-v12.hdf5\")\n",
        "with h5py.File(\"endo3_bbhpop-LIGO-T2100113-v12.hdf5\", \"r\") as ff:\n",
        "    data = ff[\"injections\"]\n",
        "    found = np.zeros_like(data[\"mass1_source\"][()], dtype=bool)\n",
        "    for key in data:\n",
        "        if \"ifar\" in key.lower():\n",
        "            found = found | (data[key][()] > 1)\n",
        "    _data[\"prior\"] /= xp.asarray(data[\"redshift_sampling_pdf\"][found])\n",
        "old_vt = ResamplingVT(model=model, data=_data, n_events=69)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13:14 bilby INFO    : Loading VT data from endo3_bbhpop-LIGO-T2100113-v12.hdf5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we generate new samples from a trained GMM to set up the new resampling method."
      ],
      "metadata": {
        "id": "5KhAoUUCBJLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gmms, _ = build_gmms(n_components=n_components, kinds=[\"empirical\"])\n",
        "new_samples_ = draw_samples(scale_method=\"empirical\")\n",
        "for key in [\"total_generated\", \"analysis_time\"]:\n",
        "    new_samples_[key] = _data[key]\n",
        "new_samples_[\"prior\"] = xp.exp(\n",
        "    powerlaw_ln_pdf(\n",
        "        new_samples_[\"mass_1\"],\n",
        "        injection_parameters[\"alpha\"],\n",
        "        injection_parameters[\"minimum_mass\"],\n",
        "        injection_parameters[\"maximum_mass\"]\n",
        "    ) + powerlaw_ln_pdf(\n",
        "        new_samples_[\"mass_ratio\"],\n",
        "        injection_parameters[\"beta\"],\n",
        "        injection_parameters[\"minimum_mass\"] / new_samples_[\"mass_1\"],\n",
        "        1\n",
        "    )\n",
        "    - np.log(4)\n",
        ")\n",
        "\n",
        "model = Model([two_component_primary_mass_ratio, iid_spin])\n",
        "new_resampling_vt = ResamplingVT(model=model, data=new_samples_, n_events=69)"
      ],
      "metadata": {
        "id": "fdTw19FLXjqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the prior distribution we will use for the analysis here and the class that will evaluate the population-averaged sensitivity."
      ],
      "metadata": {
        "id": "YaL173dBB2U6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxVM2XNOOOrd"
      },
      "source": [
        "priors = bilby.core.prior.PriorDict(conversion_function=prior_conversion)\n",
        "priors[\"alpha\"] = bilby.core.prior.Uniform(-2, 8, latex_label=\"$\\\\alpha$\")\n",
        "priors[\"beta\"] = bilby.core.prior.Uniform(-2, 8, latex_label=\"$\\\\beta$\")\n",
        "priors[\"mmin\"] = bilby.core.prior.Uniform(2, 10, latex_label=\"$m_{\\\\min}$\")\n",
        "priors[\"mmax\"] = bilby.core.prior.Uniform(50, 100, latex_label=\"$m_{\\\\max}$\")\n",
        "priors[\"lam\"] = bilby.core.prior.Uniform(0, 1, latex_label=\"$\\\\lambda_{m}$\")\n",
        "priors[\"mpp\"] = bilby.core.prior.Uniform(20, 50, latex_label=\"$\\\\mu_{m}$\")\n",
        "priors[\"sigpp\"] = bilby.core.prior.Uniform(0, 10, latex_label=\"$\\\\sigma_{m}$\")\n",
        "priors[\"amax\"] = 1\n",
        "priors[\"mu_chi\"] = bilby.core.prior.Uniform(0, 1, latex_label=\"$\\\\mu_{\\\\chi}$\")\n",
        "priors[\"sigma_chi\"] = bilby.core.prior.Uniform(0, 0.25, latex_label=\"$\\\\sigma^{2}_{\\\\chi}$\")\n",
        "priors[\"alpha_chi\"] = bilby.core.prior.Constraint(0, 1e6, latex_label=\"$\\\\alpha_{\\\\chi}$\")\n",
        "priors[\"beta_chi\"] = bilby.core.prior.Constraint(0, 1e6, latex_label=\"$\\\\beta_{\\\\chi}$\")\n",
        "priors[\"xi_spin\"] = bilby.core.prior.Uniform(0, 1, latex_label=\"$\\\\xi$\")\n",
        "priors[\"sigma_spin\"] = bilby.core.prior.Uniform(0, 4, latex_label=\"$\\\\sigma_{s}$\")\n",
        "prior_samples = pd.DataFrame(priors.sample(5000))\n",
        "\n",
        "prior_samples = prior_conversion(prior_samples)\n",
        "\n",
        "\n",
        "class GMMVT:\n",
        "    def __init__(self, n_samples, n_events, method, gmms, sampling_function, injection_parameters):\n",
        "        self.n_samples = n_samples\n",
        "        self.n_events = n_events\n",
        "        self.method = method\n",
        "        self.gmms = gmms\n",
        "        self.injection_parameters = injection_parameters\n",
        "        self.sampling_function = sampling_function\n",
        "\n",
        "    def __call__(self, parameters):\n",
        "        mu, n_effective = self.detection_efficiency(parameters)\n",
        "        if n_effective < 4 * self.n_events:\n",
        "            return np.inf\n",
        "        vt_factor = mu / np.exp((3 + self.n_events) / 2 / n_effective)\n",
        "        return vt_factor\n",
        "    \n",
        "    def detection_efficiency(self, parameters):\n",
        "        parameters[\"alpha\"] *= -1\n",
        "        if \"mmin\" not in parameters:\n",
        "            parameters[\"mmin\"] = parameters.pop(\"minimum_mass\")\n",
        "        if \"mmax\" not in parameters:\n",
        "            parameters[\"mmax\"] = parameters.pop(\"maximum_mass\")\n",
        "        \n",
        "        data = self.sampling_function(parameters, self.n_samples)\n",
        "        ln_pdets = ln_pdet(data, self.injection_parameters, self.gmms, self.method)\n",
        "        weights = xp.exp(ln_pdets)\n",
        "        mu = float(xp.mean(weights))\n",
        "        neff = xp.sum(weights) ** 2 / xp.sum(weights ** 2)\n",
        "        return mu, neff\n",
        "\n",
        "\n",
        "def sample_model(parameters, n_samples):\n",
        "    data = dict()\n",
        "    n_peak = int(n_samples * parameters[\"lam\"])\n",
        "    n_powerlaw = n_samples - n_peak\n",
        "    data[\"mass_1\"] = xp.random.permutation(np.concatenate([\n",
        "        powerlaw_sample(parameters[\"alpha\"], parameters[\"mmin\"], parameters[\"mmax\"], n_powerlaw),\n",
        "        truncnorm_sample(n_samples=n_peak, minimum=parameters[\"mmin\"], maximum=100, mu=parameters[\"mpp\"], sigma=parameters[\"sigpp\"])\n",
        "    ]))\n",
        "    data[\"mass_ratio\"] = powerlaw_sample(parameters[\"beta\"], parameters[\"mmin\"] / data[\"mass_1\"], 1, n_samples)\n",
        "    for key in [\"a_1\", \"a_2\"]:\n",
        "        data[key] = (xp.random.beta(parameters[\"alpha_chi\"], parameters[\"beta_chi\"], n_samples) * 0.99998 + 0.00001) * parameters.get(\"amax\", 1)\n",
        "    n_normal = int(n_samples * parameters[\"xi_spin\"])\n",
        "    n_isotropic = n_samples - n_normal\n",
        "    for key in [\"cos_tilt_1\", \"cos_tilt_2\"]:\n",
        "        data[key] = xp.concatenate([\n",
        "            xp.random.uniform(-1, 1, n_isotropic),\n",
        "            truncnorm_sample(n_samples=n_normal, mu=1, sigma=parameters[\"sigma_spin\"], minimum=-1, maximum=1)\n",
        "        ])\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate $P_{\\rm det}$ using the two injection resampling methods."
      ],
      "metadata": {
        "id": "GvJ7xDE6B9J9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpt2mVuttf_8"
      },
      "source": [
        "pdets = list()\n",
        "n_effs_old = list()\n",
        "pdets_rw = list()\n",
        "n_effs_old_rw = list()\n",
        "for ii in tqdm(range(len(prior_samples[:]))):\n",
        "    parameters = dict(prior_samples.iloc[ii])\n",
        "    if \"mmin\" not in parameters:\n",
        "        parameters[\"mmin\"] = parameters.pop(\"minimum_mass\")\n",
        "    if \"mmax\" not in parameters:\n",
        "        parameters[\"mmax\"] = parameters.pop(\"maximum_mass\")\n",
        "    mu, var = old_vt.detection_efficiency(parameters=parameters)\n",
        "    if mu > 0:\n",
        "        pdets.append(mu)\n",
        "        n_effs_old.append(mu ** 2 / var)\n",
        "    else:\n",
        "        pdets.append(np.nan)\n",
        "        n_effs_old.append(np.nan)\n",
        "    mu, var = new_resampling_vt.detection_efficiency(parameters=parameters)\n",
        "    if mu > 0:\n",
        "        pdets_rw.append(mu)\n",
        "        n_effs_old_rw.append(mu ** 2 / var)\n",
        "    else:\n",
        "        pdets_rw.append(np.nan)\n",
        "        n_effs_old_rw.append(np.nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate $P_{\\rm det}$ for our new method."
      ],
      "metadata": {
        "id": "zh13CLPoCIBx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FDVRkl2R52Q"
      },
      "source": [
        "n_samples = 10000\n",
        "method = \"empirical\"\n",
        "\n",
        "ln_pdets = dict()\n",
        "n_effs = dict()\n",
        "_ln_pdets = list()\n",
        "_n_effs = list()\n",
        "gmm_vt = GMMVT(\n",
        "    n_samples,\n",
        "    n_events=69,\n",
        "    method=\"empirical\",\n",
        "    gmms=gmms,\n",
        "    sampling_function=sample_model,\n",
        "    injection_parameters=injection_parameters\n",
        ")\n",
        "for ii in tqdm(range(len(prior_samples[:]))):\n",
        "    parameters = dict(prior_samples.iloc[ii])\n",
        "    if \"mmin\" not in parameters:\n",
        "        parameters[\"mmin\"] = parameters.pop(\"minimum_mass\")\n",
        "    if \"mmax\" not in parameters:\n",
        "        parameters[\"mmax\"] = parameters.pop(\"maximum_mass\")\n",
        "    mu, n_eff = gmm_vt.detection_efficiency(parameters=parameters)\n",
        "    _ln_pdets.append(np.log(mu))\n",
        "    _n_effs.append(n_eff)\n",
        "_ln_pdets = np.array(_ln_pdets)\n",
        "_n_effs = np.array(_n_effs)\n",
        "ln_pdets[n_components] = _ln_pdets\n",
        "n_effs[n_components] = _n_effs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the difference between the $P_{\\rm det}$ computed with the new methods and the two injection resampling method.\n",
        "\n",
        "The similarity in the trends between the original samples and the samples drawn from the GMM fit demsontrates that the differences are due to the difference in evaluation method, rather than loss of information in the GMM fitting."
      ],
      "metadata": {
        "id": "d3VrncU1Cb8w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mq_jKTI4X_t"
      },
      "source": [
        "for param in list(priors.keys()):\n",
        "    plt.figure(figsize=(12, 7.5))\n",
        "    plt.scatter(\n",
        "        prior_samples[param],\n",
        "        np.log10(np.exp(_ln_pdets - np.log(pdets))),\n",
        "        label=\"Old\",\n",
        "        s=20,\n",
        "        marker=\"x\"\n",
        "    )\n",
        "    plt.scatter(\n",
        "        prior_samples[param],\n",
        "        np.log10(np.exp(_ln_pdets - np.log(pdets_rw))),\n",
        "        label=\"New Samples\",\n",
        "        s=20,\n",
        "        marker=\"+\"\n",
        "    )\n",
        "    plt.xlabel(priors[param].latex_label)\n",
        "    plt.ylabel(\"$\\\\Delta \\\\log_{10} P_{\\\\rm det}$\")\n",
        "    if param in [\"alpha_chi\", \"beta_chi\"]:\n",
        "        plt.xscale(\"log\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the distribution of selection functions for the three methods for the non-singular and singular spin distributions.\n",
        "\n",
        "This is Figure 3 of the paper."
      ],
      "metadata": {
        "id": "1GBfzaEOCxHL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fKbbNx56uup"
      },
      "source": [
        "non_singular = (prior_samples[\"alpha_chi\"].values > 1) & (prior_samples[\"beta_chi\"].values > 1)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, figsize=(8, 10))\n",
        "\n",
        "plt.sca(axes[0])\n",
        "plt.hist(np.log10(pdets)[non_singular], bins=np.linspace(-4, -1, 100), density=True, histtype=\"step\")\n",
        "plt.hist(np.log10(pdets_rw)[non_singular], bins=np.linspace(-4, -1, 100), density=True, histtype=\"step\")\n",
        "plt.hist(np.log10(np.exp(_ln_pdets[non_singular])), bins=np.linspace(-4, -1, 100), density=True, histtype=\"step\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"$\\\\log_{10} P_{\\\\rm det}$\")\n",
        "plt.ylabel(\"$p(\\\\log_{10} P_{\\\\rm det})$\")\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plt.hist(np.log10(pdets)[~non_singular], bins=np.linspace(-9, -1, 100), density=True, histtype=\"step\", label=\"Old\")\n",
        "plt.hist(np.log10(pdets_rw)[~non_singular], bins=np.linspace(-9, -1, 100), density=True, histtype=\"step\", label=\"New Samples\")\n",
        "plt.hist(np.log10(np.exp(_ln_pdets[~non_singular])), bins=np.linspace(-9, -1, 100), density=True, histtype=\"step\", label=\"New\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"$\\\\log_{10} P_{\\\\rm det}$\")\n",
        "plt.ylabel(\"$p(\\\\log_{10} P_{\\\\rm det})$\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Pdet.pdf\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the original samples weighted by the selection function for the different methods.\n",
        "\n",
        "Using the arbitrary scale more clearly shows the difference for the spin magnitude parameters. Setting `density=True` demonstrates that there is no observable difference for many of the parameters.\n",
        "\n",
        "One of these is Figure 4 in the paper."
      ],
      "metadata": {
        "id": "8v4ohSObDAee"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIPFrxzel-0A"
      },
      "source": [
        "for parameter in priors.keys():\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    if parameter in [\"alpha_chi\", \"beta_chi\"]:\n",
        "        points = np.log10(prior_samples[parameter])\n",
        "    else:\n",
        "        points = prior_samples[parameter]\n",
        "    for ii, samples_ in enumerate([pdets, pdets_rw, np.exp(_ln_pdets)]):\n",
        "        plt.hist(\n",
        "            points,\n",
        "            weights=samples_,\n",
        "            density=False,\n",
        "            histtype=\"step\",\n",
        "            color=f\"C{ii}\",\n",
        "            bins=100,\n",
        "            label=[\"Old\", \"New Samples\", \"New\"][ii]\n",
        "        )\n",
        "    if parameter in [\"alpha_chi\", \"beta_chi\"]:\n",
        "        plt.xlabel(f\"$\\\\log_{{10}}\\,${priors[parameter].latex_label}\")\n",
        "    else:\n",
        "        plt.xlabel(priors[parameter].latex_label)\n",
        "    plt.ylabel(\"$P_{\\\\rm det}$ [arbitrary units]\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"Pdet_{parameter}.pdf\")\n",
        "    # plt.yscale(\"log\")\n",
        "    # plt.ylim(-5, 1)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the number of effective samples as a function of population parameter for the three methods.\n",
        "\n",
        "Two of these are used in Figure 5 of the paper."
      ],
      "metadata": {
        "id": "63PYkhEVCN4o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WVo7Q8Qc2Od"
      },
      "source": [
        "method = \"empirical\"\n",
        "for param in list(priors.keys()):\n",
        "    plt.figure(figsize=(12, 7.5))\n",
        "    for ii, _neffs in enumerate([n_effs_old, n_effs_old_rw, _n_effs]):\n",
        "        plt.scatter(\n",
        "            prior_samples[param],\n",
        "            _neffs,\n",
        "            s=20,\n",
        "            label=[\"Old\", \"New Samples\", \"New\"][ii],\n",
        "            color=f\"C{ii}\",\n",
        "        )\n",
        "    plt.ylim(1, 10 * max(n_effs_old))\n",
        "    plt.axhline(4 * 69, color=\"k\", linestyle=\"--\", label=\"Threshold\")\n",
        "    if param in priors:\n",
        "        plt.xlabel(priors[param].latex_label)\n",
        "    else:\n",
        "        plt.xlabel(param.replace(\"_\", \" \"))\n",
        "    if param in [\"alpha_chi\", \"beta_chi\"]:\n",
        "        plt.xscale(\"log\")\n",
        "    plt.ylabel(\"$N_{\\\\rm eff}$\")\n",
        "    plt.legend(loc=\"upper left\", ncol=2)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"neff_{param}.pdf\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F36nxv2ambnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3850c0-4ed6-4077-92cc-9e12e061eb62"
      },
      "source": [
        "for meth, neff in zip([\"Old\", \"New Samples\", \"New\"], [n_effs_old, n_effs_old_rw, _neffs]):\n",
        "    print(\n",
        "        f\"For the {meth} method we reject \"\n",
        "        f\"{np.mean(np.array(neff)[non_singular] < 4 * 69) * 100:.1f}% of non-singular samples, \"\n",
        "        f\"{np.mean(np.array(neff)[~non_singular] < 4 * 69) * 100:.1f}% of singular samples, \"\n",
        "        f\"and {np.mean(np.array(neff) < 4 * 69) * 100:.1f}% of all samples.\"\n",
        "    )\n",
        "\n",
        "\n",
        "print(f\"New method has more effective samples (smaller uncertainty) in {np.mean(np.array(_neffs) > np.array(n_effs_old)) * 100:.1f}% of the space.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the Old method we reject 12.6% of non-singular samples, 53.1% of singular samples, and 44.2% of all samples.\n",
            "For the New Samples method we reject 12.9% of non-singular samples, 60.5% of singular samples, and 49.9% of all samples.\n",
            "For the New method we reject 0.1% of non-singular samples, 0.4% of singular samples, and 0.3% of all samples.\n",
            "New method has more effective samples (smaller uncertainty) in 82.6% of the space.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run population inference\n",
        "\n",
        "Now we can run population inference with our three selection function evaluators.\n",
        "\n",
        "Here I mount my drive and use a prepared file. Other users will have to collate the data from the official releases.\n",
        "\n",
        "For this demonstration I use the `bilby-mcmc` sampler with fairly aggresive settings as all we are interested in are the observed spectra and don't mind having correlated MCMC samples."
      ],
      "metadata": {
        "id": "s5prS-wTDYKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gwpopulation_pipe.utils import MinimumEffectiveSamplesLikelihood\n",
        "from gwpopulation.conversions import convert_to_beta_parameters\n",
        "from gwpopulation.hyperpe import HyperparameterLikelihood\n",
        "\n",
        "likelihood_class = MinimumEffectiveSamplesLikelihood\n",
        "\n",
        "posteriors = pd.read_pickle(\"drive/MyDrive/science/GWTC-3_posteriors.pkl\")\n",
        "\n",
        "run_model = Model([two_component_primary_mass_ratio, iid_spin])\n",
        "\n",
        "likelihood_original = likelihood_class(\n",
        "    posteriors=deepcopy(posteriors),\n",
        "    hyper_prior=deepcopy(run_model),\n",
        "    conversion_function=convert_to_beta_parameters,\n",
        "    selection_function=old_vt\n",
        ")\n",
        "\n",
        "likelihood_new_samples = likelihood_class(\n",
        "    posteriors=deepcopy(posteriors),\n",
        "    hyper_prior=deepcopy(run_model),\n",
        "    conversion_function=convert_to_beta_parameters,\n",
        "    selection_function=new_resampling_vt\n",
        ")\n",
        "\n",
        "likelihood_new = likelihood_class(\n",
        "    posteriors=deepcopy(posteriors),\n",
        "    hyper_prior=deepcopy(run_model),\n",
        "    conversion_function=convert_to_beta_parameters,\n",
        "    selection_function=gmm_vt\n",
        ")"
      ],
      "metadata": {
        "id": "a--28YkcgbTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_new = bilby.run_sampler(\n",
        "    likelihood=likelihood_new, priors=priors,\n",
        "    nsamples=500, proposal_cycle=\"defaultnoKDnoNFnoUN\",\n",
        "    L1Steps=10, L2Steps=1, printdt=20, check_point_delta_t=180,\n",
        "    fixed_tau=2, fixed_discard=100,\n",
        "    nlive=250, label=\"new\", resume=False, sampler=\"bilby_mcmc\"\n",
        ")\n",
        "result_original = bilby.run_sampler(\n",
        "    likelihood=likelihood_original, priors=priors,\n",
        "    nsamples=500, proposal_cycle=\"defaultnoKDnoNFnoUN\",\n",
        "    L1Steps=10, L2Steps=1, printdt=20, check_point_delta_t=180,\n",
        "    fixed_tau=2, fixed_discard=100,\n",
        "    nlive=250, label=\"original\", resume=False, sampler=\"bilby_mcmc\"\n",
        ")\n",
        "result_new_samples = bilby.run_sampler(\n",
        "    likelihood=likelihood_new_samples, priors=priors,\n",
        "    nsamples=500, proposal_cycle=\"defaultnoKDnoNFnoUN\",\n",
        "    L1Steps=10, L2Steps=1, printdt=20, check_point_delta_t=180,\n",
        "    fixed_tau=2, fixed_discard=100,\n",
        "    nlive=250, label=\"new_samples\", resume=False, sampler=\"bilby_mcmc\"\n",
        ")"
      ],
      "metadata": {
        "id": "Xm-A3vH64DVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the inferred primary mass and spin magnitude spectra.\n",
        "\n",
        "This is Figure 6 in the paper."
      ],
      "metadata": {
        "id": "4JTgI6HlD4o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aa = np.linspace(1e-5, 1 - 1e-5, 10000)\n",
        "ms = np.linspace(2, 100, 10000)\n",
        "\n",
        "\n",
        "def _truncnorm_pdf(xx, mu, sigma, high, low):\n",
        "    from scipy.special import erf, erfinv\n",
        "    norm = 2 ** 0.5 / xp.pi ** 0.5 / sigma\n",
        "    norm /= erf((high - mu) / 2 ** 0.5 / sigma) + erf((mu - low) / 2 ** 0.5 / sigma)\n",
        "    prob = np.exp(-np.power(xx - mu, 2) / (2 * sigma ** 2))\n",
        "    prob *= norm\n",
        "    prob *= (xx <= high) & (xx >= low)\n",
        "    return prob\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "fig, axes = plt.subplots(nrows=2, figsize=(8, 10))\n",
        "for jj, result in enumerate([result_original, result_new_samples, result_new]):\n",
        "    a_lines = list()\n",
        "    m_lines = list()\n",
        "    for ii in trange(len(result.posterior)):\n",
        "        parameters = dict(result.posterior.iloc[ii])\n",
        "        parameters = convert_to_beta_parameters(parameters)[0]\n",
        "        prob = np.exp(\n",
        "            (parameters[\"alpha_chi\"] - 1) * np.log(aa)\n",
        "            + (parameters[\"beta_chi\"] - 1) * np.log(1 - aa)\n",
        "            - scbetaln(parameters[\"alpha_chi\"], parameters[\"beta_chi\"])\n",
        "        )\n",
        "        a_lines.append(prob)\n",
        "        prob = (\n",
        "            (1 - parameters[\"lam\"]) * np.exp(powerlaw_ln_pdf(ms, -parameters[\"alpha\"], parameters[\"mmin\"], parameters[\"mmax\"]))\n",
        "            + parameters[\"lam\"] * _truncnorm_pdf(ms, parameters[\"mpp\"], parameters[\"sigpp\"], 100, parameters[\"mmin\"])\n",
        "        )\n",
        "        m_lines.append(prob)\n",
        "    plt.sca(axes[0])\n",
        "    plt.plot(ms, np.mean(m_lines, axis=0), color=f\"C{jj}\", label=result.label.replace(\"_\", \" \").title())\n",
        "    plt.fill_between(ms, np.percentile(m_lines, 5, axis=0), np.percentile(m_lines, 95, axis=0), color=f\"C{jj}\", alpha=0.1)\n",
        "    plt.sca(axes[1])\n",
        "    plt.plot(aa, np.mean(a_lines, axis=0), color=f\"C{jj}\", label=result.label.replace(\"_\", \" \").title())\n",
        "    plt.fill_between(aa, np.percentile(a_lines, 5, axis=0), np.percentile(a_lines, 95, axis=0), color=f\"C{jj}\", alpha=0.1)\n",
        "plt.sca(axes[0])\n",
        "plt.yscale(\"log\")\n",
        "plt.xlim(2, 100)\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel(\"$m_{1}\\,[M_{\\\\odot}]$\")\n",
        "plt.ylabel(\"$p(m_{1})$\")\n",
        "plt.sca(axes[1])\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 5)\n",
        "plt.xlabel(\"$a$\")\n",
        "plt.ylabel(\"$p(a)$\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"ppd-m1-a.pdf\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "keIH31WvjUae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x3Rcmfso3B2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}